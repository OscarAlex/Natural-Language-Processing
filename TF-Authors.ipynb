{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "developed-andorra",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import io\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "clinical-tomato",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def json_to_dict(file):\n",
    "    diction= open(file)\n",
    "    return json.load(diction)\n",
    "\n",
    "def plain_to_dict(file):\n",
    "    f= open(file, encoding=\"utf8\")\n",
    "    return json.load(f)\n",
    "\n",
    "#Geology\n",
    "geo1= json_to_dict('geologic_processes.json')\n",
    "geo2= plain_to_dict('age_of_earth')\n",
    "#Paleontolgy\n",
    "paleo= plain_to_dict('paleontology.json')\n",
    "#Cosmology\n",
    "cosmo= plain_to_dict('cosmology')\n",
    "#Ecology\n",
    "eco= plain_to_dict('ecology_conservation')\n",
    "#Faith and science\n",
    "faith= plain_to_dict('faith_and_science')\n",
    "#Intelligent desgin\n",
    "design= plain_to_dict('inteligentDesign.json')\n",
    "#Biology\n",
    "bio= plain_to_dict('Biology.json')\n",
    "#Other sciences\n",
    "\n",
    "#Join Geo\n",
    "geo= {\"geology\": list(geo1.values())[0]+list(geo2.values())[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "everyday-procedure",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(rgx_list, text):\n",
    "    new_text = text.lower()\n",
    "    for rgx_match in rgx_list:\n",
    "        new_text = re.sub(rgx_match, '', new_text)   \n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "equivalent-browse",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_arts= []\n",
    "all_labels= []\n",
    "\n",
    "for count,top in enumerate([geo,paleo,cosmo,eco,faith,design]):\n",
    "    arts= []\n",
    "\n",
    "    for art in list(top.values())[0]:#[:30]:\n",
    "        arts.append(art['author'])\n",
    "    labels= [count]*len(arts)\n",
    "    \n",
    "    all_arts.append(arts)\n",
    "    all_labels.append(labels)\n",
    "    #print(arts)\n",
    "    #print(labels)\n",
    "\n",
    "\n",
    "all_arts= [item for sublist in all_arts for item in sublist]\n",
    "all_labels= [item for sublist in all_labels for item in sublist]\n",
    "\n",
    "patterns= [#r'[\\t\\n\\r\\f\\v\\d]', r'(.+doi.+(Summary\\. ?|\\n)|.+DOI.+(Summary\\. ?|\\n))',\n",
    "           #r'\\[[\\w\\.;,\\- ]+\\]', r'\\([\\w\\.;,\\- ]+\\)', r'“[\\w+]”', r'[\\w+]”', r'“[\\w+]',\n",
    "           #r'[F|f]ig[\\.\\w]+ [\\w,\\- ]+|see [F|f]ig[\\.\\w]+ [\\w,\\- ]+', r'[\\\"]',\n",
    "           #r'[.,\\/#!$%\\^&\\*;:{}=\\-\\[\\]_`~()“”]', r'[\\']',r'[\\’]',\n",
    "           #r'WHAT THIS ARTICLE IS ABOUT|ABSTRACT|INTRODUCTION|ACKNOWLEDGMENTS?|CONCLUSIONS?|Conclusions?|SUMMARY|DISCUSSION|Geoscience Research Institute',\n",
    "           #r'\\bthe\\b',r'\\bof\\b',r'\\bin\\b',r'\\bare\\b',r'\\bthe[m|y]\\b',r'\\bi(s|f|t)\\b',r'\\ba(n|ll|nd|t)?\\b',r'\\bto\\b',r'\\bwhich\\b',r'\\bby\\b',\n",
    "           #r'\\bf?or\\b',r'\\bthose\\b',r'\\bfrom\\b',r'\\bto\\b',r'\\bbut\\b',r'\\bthese\\b',r'>',r'\\bwith\\b',r'\\xa0'\n",
    "            r'[\\w]+ ?[\\w\\.]? '\n",
    "            ]\n",
    "\n",
    "all_cleaned= []\n",
    "for art in all_arts:\n",
    "    #art= re.split('ENDNOTES|Endnotes|REFERENCES|References|Footnotes|FOR FURTHER STUDY|LITERATURE CITED', art)[0]\n",
    "    #art= clean_text(patterns, art)\n",
    "    all_cleaned.append(clean_text(patterns, art))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "universal-politics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"rt= 'Timothy G. Standish'\\n#new_text = re.sub(r'[\\\\w]+ \\\\w\\\\. ', '', rt)\\nnew_text = re.sub(r'[\\\\w]+ ?[\\\\w\\\\.]? ', '', rt)\\nprint(new_text)\""
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"rt= 'Timothy G. Standish'\n",
    "#new_text = re.sub(r'[\\w]+ \\w\\. ', '', rt)\n",
    "new_text = re.sub(r'[\\w]+ ?[\\w\\.]? ', '', rt)\n",
    "print(new_text)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "sharp-forest",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sets\n",
    "x_train, x_test, y_train, y_test= train_test_split(all_cleaned, all_labels, test_size=0.25)\n",
    "\n",
    "training_labels= np.array(y_train)\n",
    "testing_labels= np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "endangered-paper",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variables\n",
    "#vocab_size= 642\n",
    "vocab_size= 75\n",
    "max_length= 32\n",
    "trunc_type= \"post\"\n",
    "padding_type= \"post\"\n",
    "oov_tok= \"<OOV>\"\n",
    "\n",
    "#Tokenize\n",
    "tokenizer= Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "word_index= tokenizer.word_index\n",
    "\n",
    "#Train sentences\n",
    "training_sequences= tokenizer.texts_to_sequences(x_train)\n",
    "training_padded= pad_sequences(training_sequences, maxlen=max_length,\n",
    "                               padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "#Test sentences\n",
    "testing_sequences= tokenizer.texts_to_sequences(x_test)\n",
    "testing_padded= pad_sequences(testing_sequences, maxlen=max_length, \n",
    "                              padding=padding_type, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "martial-audio",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 32, 16)            1200      \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_7 ( (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 24)                408       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 25        \n",
      "=================================================================\n",
      "Total params: 1,633\n",
      "Trainable params: 1,633\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim= 16\n",
    "\n",
    "#Keras model\n",
    "model= tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(24, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "interstate-eligibility",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 300 samples, validate on 101 samples\n",
      "Epoch 1/30\n",
      "300/300 - 1s - loss: 0.6079 - accuracy: 0.1533 - val_loss: 0.5083 - val_accuracy: 0.1485\n",
      "Epoch 2/30\n",
      "300/300 - 0s - loss: 0.4485 - accuracy: 0.1533 - val_loss: 0.3397 - val_accuracy: 0.1485\n",
      "Epoch 3/30\n",
      "300/300 - 0s - loss: 0.2690 - accuracy: 0.1533 - val_loss: 0.1329 - val_accuracy: 0.1485\n",
      "Epoch 4/30\n",
      "300/300 - 0s - loss: 0.0566 - accuracy: 0.1533 - val_loss: -1.0910e-01 - val_accuracy: 0.1485\n",
      "Epoch 5/30\n",
      "300/300 - 0s - loss: -1.9450e-01 - accuracy: 0.1533 - val_loss: -3.9325e-01 - val_accuracy: 0.1485\n",
      "Epoch 6/30\n",
      "300/300 - 0s - loss: -4.9019e-01 - accuracy: 0.1533 - val_loss: -7.3983e-01 - val_accuracy: 0.1485\n",
      "Epoch 7/30\n",
      "300/300 - 0s - loss: -8.5322e-01 - accuracy: 0.1533 - val_loss: -1.1544e+00 - val_accuracy: 0.1485\n",
      "Epoch 8/30\n",
      "300/300 - 0s - loss: -1.2852e+00 - accuracy: 0.1533 - val_loss: -1.6412e+00 - val_accuracy: 0.1485\n",
      "Epoch 9/30\n",
      "300/300 - 0s - loss: -1.7764e+00 - accuracy: 0.1533 - val_loss: -2.2088e+00 - val_accuracy: 0.1485\n",
      "Epoch 10/30\n",
      "300/300 - 0s - loss: -2.3667e+00 - accuracy: 0.1533 - val_loss: -2.8660e+00 - val_accuracy: 0.1485\n",
      "Epoch 11/30\n",
      "300/300 - 0s - loss: -3.0462e+00 - accuracy: 0.1533 - val_loss: -3.6289e+00 - val_accuracy: 0.1485\n",
      "Epoch 12/30\n",
      "300/300 - 0s - loss: -3.8366e+00 - accuracy: 0.1533 - val_loss: -4.4924e+00 - val_accuracy: 0.1485\n",
      "Epoch 13/30\n",
      "300/300 - 0s - loss: -4.7032e+00 - accuracy: 0.1533 - val_loss: -5.4828e+00 - val_accuracy: 0.1485\n",
      "Epoch 14/30\n",
      "300/300 - 0s - loss: -5.7044e+00 - accuracy: 0.1533 - val_loss: -6.5751e+00 - val_accuracy: 0.1485\n",
      "Epoch 15/30\n",
      "300/300 - 0s - loss: -6.8075e+00 - accuracy: 0.1533 - val_loss: -7.8138e+00 - val_accuracy: 0.1485\n",
      "Epoch 16/30\n",
      "300/300 - 0s - loss: -8.0605e+00 - accuracy: 0.1533 - val_loss: -9.2282e+00 - val_accuracy: 0.1485\n",
      "Epoch 17/30\n",
      "300/300 - 0s - loss: -9.4924e+00 - accuracy: 0.1533 - val_loss: -1.0808e+01 - val_accuracy: 0.1485\n",
      "Epoch 18/30\n",
      "300/300 - 0s - loss: -1.1085e+01 - accuracy: 0.1533 - val_loss: -1.2584e+01 - val_accuracy: 0.1485\n",
      "Epoch 19/30\n",
      "300/300 - 0s - loss: -1.2836e+01 - accuracy: 0.1533 - val_loss: -1.4553e+01 - val_accuracy: 0.1485\n",
      "Epoch 20/30\n",
      "300/300 - 0s - loss: -1.4819e+01 - accuracy: 0.1533 - val_loss: -1.6738e+01 - val_accuracy: 0.1485\n",
      "Epoch 21/30\n",
      "300/300 - 0s - loss: -1.7026e+01 - accuracy: 0.1533 - val_loss: -1.9171e+01 - val_accuracy: 0.1485\n",
      "Epoch 22/30\n",
      "300/300 - 0s - loss: -1.9416e+01 - accuracy: 0.1533 - val_loss: -2.1867e+01 - val_accuracy: 0.1485\n",
      "Epoch 23/30\n",
      "300/300 - 0s - loss: -2.2129e+01 - accuracy: 0.1533 - val_loss: -2.4759e+01 - val_accuracy: 0.1485\n",
      "Epoch 24/30\n",
      "300/300 - 0s - loss: -2.5041e+01 - accuracy: 0.1533 - val_loss: -2.7981e+01 - val_accuracy: 0.1485\n",
      "Epoch 25/30\n",
      "300/300 - 0s - loss: -2.8276e+01 - accuracy: 0.1533 - val_loss: -3.1503e+01 - val_accuracy: 0.1485\n",
      "Epoch 26/30\n",
      "300/300 - 0s - loss: -3.1740e+01 - accuracy: 0.1533 - val_loss: -3.5355e+01 - val_accuracy: 0.1485\n",
      "Epoch 27/30\n",
      "300/300 - 0s - loss: -3.5596e+01 - accuracy: 0.1533 - val_loss: -3.9492e+01 - val_accuracy: 0.1485\n",
      "Epoch 28/30\n",
      "300/300 - 0s - loss: -3.9669e+01 - accuracy: 0.1533 - val_loss: -4.4005e+01 - val_accuracy: 0.1485\n",
      "Epoch 29/30\n",
      "300/300 - 0s - loss: -4.4148e+01 - accuracy: 0.1533 - val_loss: -4.8865e+01 - val_accuracy: 0.1485\n",
      "Epoch 30/30\n",
      "300/300 - 0s - loss: -4.9053e+01 - accuracy: 0.1533 - val_loss: -5.4066e+01 - val_accuracy: 0.1485\n"
     ]
    }
   ],
   "source": [
    "#Train\n",
    "num_epochs= 30\n",
    "\n",
    "history= model.fit(training_padded, training_labels, epochs=num_epochs,\n",
    "                   validation_data=(testing_padded, testing_labels), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "absolute-figure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75, 16)\n"
     ]
    }
   ],
   "source": [
    "e= model.layers[0]\n",
    "weights= e.get_weights()[0]\n",
    "print(weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "insured-spread",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: '<OOV>', 2: 'roth', 3: 'brown', 4: 'gibson', 5: 'standish', 6: 'nalin', 7: 'ching', 8: 'clausen', 9: 'brand', 10: 'tkachuck', 11: 'esperante', 12: 'giem', 13: 'hasel', 14: 'jr', 15: 'shea', 16: 'davidson', 17: 'bergman', 18: 'chadwick', 19: 'zuill', 20: 'baldwin', 21: 'coffin', 22: 'suzuki', 23: 'johns', 24: 'wolfe', 25: 'guliuzza', 26: 'javor', 27: 'schafer', 28: 'doukhan', 29: 'rodríguez', 30: 'fraser', 31: 'neufeld', 32: 'kootsey', 33: 'sciarabba', 34: 'phillips', 35: 'mclain', 36: 'naledi', 37: 'frair', 38: 'wheeler', 39: 'kissinger', 40: 'biaggi', 41: 'smith', 42: 'hart', 43: 'younker', 44: 'clark', 45: 'ford', 46: 'miller', 47: 'graham', 48: 'kennedy', 49: 'aagaard', 50: 'camp', 51: 'steger', 52: 'gregor', 53: 'burdick', 54: 'wise', 55: 'snelling', 56: 'klingbeil', 57: 'lugeneal', 58: 'mccluskey', 59: 'price', 60: 'kotulla', 61: 'silva', 62: 'hayes', 63: 'dwyer', 64: 'marsh', 65: 'wood', 66: 'brandstater', 67: 'duran', 68: 'carter', 69: 'ritland', 70: 'boyle', 71: 'schoepflin', 72: 'hodges', 73: 'barnett', 74: 'abrahamson', 75: 'groot'}\n"
     ]
    }
   ],
   "source": [
    "rev_vocab= dict([(value,key) for (key,value) in word_index.items()])\n",
    "print(rev_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "copyrighted-cleaners",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TSV\n",
    "out_v= io.open('vecs-author.tsv', 'w', encoding='utf-8')\n",
    "out_m= io.open('meta-author.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for word_num in range(1, vocab_size):\n",
    "    word= rev_vocab[word_num]\n",
    "    embeddings= weights[word_num]\n",
    "    out_m.write(word + \"\\n\")\n",
    "    out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\n",
    "\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
