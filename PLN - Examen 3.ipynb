{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "competent-burton",
   "metadata": {},
   "source": [
    "# Procesamiento de Lenguaje Natural - Examen 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "every-tooth",
   "metadata": {},
   "source": [
    "En el tercer examen parcial trabajarán en equipos de 2 personas en encontrar patrones interesantes mediante técnicas de procesamiento de lenguaje natural en los datos abiertos ofrecidos por: https://www.grisda.org/ y https://pages.semanticscholar.org/coronavirus-research."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conservative-mortality",
   "metadata": {},
   "source": [
    "Si algún equipo quiere hacer el ejercicio para la semana entrante en GENSIM, ¡bienvenido sea!… de hecho, al equipo que use GENSIM le puedo dar puntos extra por el esfuerzo. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behavioral-shoot",
   "metadata": {},
   "source": [
    "## 24 de marzo (40%)\n",
    "En este primer ejercicio, crearán un dataset con “todos” los artículos en https://www.grisda.org/. Todos los equipos pueden ponerse de acuerdo para crear un solo dataset. Además, pueden utilizar un crawler para agilizar el proceso. En este ejercicio utilizarán word embeddings entrenados con el dataset obtenido de GRISDA y lo visualizarán en el proyector de TF. Ese día subirán a Google Classroom el dataset y un documento explicando 4 excelentes resultados en términos de palabras relacionadas: imagen mas uno o dos buenos párrafos describiendo cada resultado. **Deben ser resultados de “alto calibre”, nada trivial.**\n",
    "\n",
    "No tienen que hacer una clasificación. Solamente el word2vec, visualizarlo en TF Projector, y encontrar patrones entre las palabras. Aparte del código que estoy adjuntando, hay otros códigos de ejemplo que pueden encontrar en la Web. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baking-globe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faith-and-Science 236\n",
      "dating 40\n",
      "paleontology 61\n",
      "otherSciences 19\n",
      "Cosmology 12\n",
      "geologic_procesess 17\n",
      "ecologyConservation 15\n",
      "intelligentDesign 20\n",
      "Biology 120\n",
      "\n",
      "Training 495\n",
      "Testing 45\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "jsonfiles = {\n",
    "    \"Faith-and-Science\": \"faith_and_science.json\",\n",
    "    \"dating\": \"dating_and_age_of_earth.json\",\n",
    "    \"paleontology\": \"paleontology.json\",\n",
    "    \"otherSciences\": \"other_sciences.json\",\n",
    "    \"Cosmology\": \"cosmology.json\",\n",
    "    \"geologic_procesess\": \"geologic_processes.json\",\n",
    "    \"ecologyConservation\": \"ecology_conservation.json\",\n",
    "    \"intelligentDesign\": \"intelligent_design.json\",\n",
    "    \"Biology\": \"biology.json\"\n",
    "}\n",
    "\n",
    "training_sentences = []; training_labels = []\n",
    "testing_sentences = []; testing_labels = []\n",
    "t = 5; i = 0\n",
    "\n",
    "for k, v in list(jsonfiles.items()):\n",
    "    with open (v, \"r\") as f:\n",
    "        grisda = json.load(f)[k]\n",
    "    training_sentences.extend([art[\"doc\"] for art in grisda[t:]])\n",
    "    testing_sentences.extend([art[\"doc\"] for art in grisda[:t]])\n",
    "    training_labels.extend([i]*len(grisda[t:]))\n",
    "    testing_labels.extend([i]*t)\n",
    "    i+=1\n",
    "    print(k, len(grisda))\n",
    "    \n",
    "training_labels = np.array(training_labels)\n",
    "testing_labels = np.array(testing_labels)\n",
    "    \n",
    "print(\"\\nTraining\", len(training_sentences))\n",
    "print(\"Testing\", len(testing_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "twenty-worse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words removed\n",
      "(1, '<OOV>')\n",
      "(2, 'the')\n",
      "(3, 'creation')\n",
      "(4, '1')\n",
      "(5, 'in')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Hyperparameters\n",
    "vocab_size = 10000\n",
    "max_length = 200\n",
    "trunc_type =\"post\"\n",
    "oov_tok = \"<OOV>\"\n",
    "\n",
    "# Remove stop words\n",
    "training_sentences = [remove_stopwords(t) for t in training_sentences]\n",
    "print(\"Stop words removed\")\n",
    "\n",
    "# Tokenize\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Sequences\n",
    "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "\n",
    "# Padding\n",
    "training_padded = pad_sequences(training_sequences, maxlen=max_length, truncating=trunc_type)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen=max_length)\n",
    "\n",
    "# Reverse word index\n",
    "reverse_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "[print(v) for i, v in enumerate(reverse_index.items()) if i < 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sixth-steps",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16/16 - 3s - loss: 2.1778 - accuracy: 0.4364 - val_loss: 2.1969 - val_accuracy: 0.1111\n",
      "Epoch 2/10\n",
      "16/16 - 0s - loss: 2.1217 - accuracy: 0.4707 - val_loss: 2.2001 - val_accuracy: 0.1111\n",
      "Epoch 3/10\n",
      "16/16 - 0s - loss: 2.0382 - accuracy: 0.4667 - val_loss: 2.2127 - val_accuracy: 0.1111\n",
      "Epoch 4/10\n",
      "16/16 - 0s - loss: 1.9225 - accuracy: 0.4667 - val_loss: 2.2529 - val_accuracy: 0.1111\n",
      "Epoch 5/10\n",
      "16/16 - 0s - loss: 1.7763 - accuracy: 0.4667 - val_loss: 2.3513 - val_accuracy: 0.1111\n",
      "Epoch 6/10\n",
      "16/16 - 0s - loss: 1.6319 - accuracy: 0.4667 - val_loss: 2.5368 - val_accuracy: 0.1111\n",
      "Epoch 7/10\n",
      "16/16 - 0s - loss: 1.5242 - accuracy: 0.4667 - val_loss: 2.7553 - val_accuracy: 0.1111\n",
      "Epoch 8/10\n",
      "16/16 - 0s - loss: 1.4604 - accuracy: 0.4667 - val_loss: 2.9133 - val_accuracy: 0.1111\n",
      "Epoch 9/10\n",
      "16/16 - 0s - loss: 1.4154 - accuracy: 0.4667 - val_loss: 2.9906 - val_accuracy: 0.1111\n",
      "Epoch 10/10\n",
      "16/16 - 0s - loss: 1.3694 - accuracy: 0.4667 - val_loss: 3.0391 - val_accuracy: 0.1111\n"
     ]
    }
   ],
   "source": [
    "# Create an embedding layer\n",
    "embedding_dim = 16\n",
    "# Train this embedding as part of a keras model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(45, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(9, activation=\"sigmoid\"),\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    training_padded, training_labels, epochs=10, \n",
    "    validation_data=(testing_padded, testing_labels), verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "polyphonic-logging",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 16)\n"
     ]
    }
   ],
   "source": [
    "#TSV\n",
    "import io\n",
    "\n",
    "# extract Weights\n",
    "e = model.layers[0]\n",
    "weights = e.get_weights()[0]\n",
    "print(weights.shape) # shape: (vocab_size, embedding_dim)\n",
    "\n",
    "out_v= io.open('vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m= io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for word_num in range(1, vocab_size):\n",
    "    word = reverse_index[word_num]\n",
    "    embeddings = weights[word_num]\n",
    "    out_m.write(word + \"\\n\")\n",
    "    out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\n",
    "\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal-graph",
   "metadata": {},
   "source": [
    "En los siguientes puntos pueden utilizar las técnicas y herramientas de procesamiento de lenguaje natural que deseen. Lo importante es mostrar cada semana **\"resultados de valor\"** para la lucha contra el flagelo del COVID-19. Entregarán 3 resultados, uno cada semana.\n",
    "\n",
    "Sus resultados servirán para que médicos o expertos en áreas relacionadas con la mitigación del COVID-19 los puedan aplicar (este es un factor _**MUY**_ importante en la calificación). Por ejemplo, les invito a ver ideas de posibles experimentos que pueden realizar cada semana en el siguiente sitio web: https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge/tasks. Además, pueden pensar en otras tareas que puedan resolver mediante procesamiento de lenguaje natural.\n",
    "\n",
    "Estas son las fechas de evaluación:\n",
    "- 30 de marzo (30%)\n",
    "- 6 de abril (30%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "productive-telescope",
   "metadata": {},
   "source": [
    "## 30 de marzo (30%)\n",
    "Presentar dos resultados ante el grupo. Subirán en esa fecha un documento explicando los resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spread-tenant",
   "metadata": {},
   "source": [
    "## 6 de abril (30%)\n",
    "Presentar dos resultados ante el grupo. Subirán en esa fecha un documento explicando los resultados."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
